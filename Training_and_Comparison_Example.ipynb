{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Comparing RL Agents (PPO vs. DQN)\n",
    "\n",
    "This notebook provides a complete, end-to-end example of using the `rl_trading_project` framework with a robust data pipeline. We will perform the following steps:\n",
    "\n",
    "1.  **Data Ingestion:** Create a high-performance DuckDB database from raw, gzipped CSV files. This simulates a real-world scenario where you have large amounts of historical data.\n",
    "2.  **Data Loading & Preparation:** Load the prepared data into a Pandas DataFrame, ready for our RL environments.\n",
    "3.  **Train a PPO Agent:** Train a Proximal Policy Optimization (PPO) agent to manage a **multi-asset portfolio**. PPO is ideal for this task due to its ability to handle continuous, multi-dimensional action spaces.\n",
    "4.  **Train a DQN Agent:** Train a Dueling Deep Q-Network (DQN) agent on a **single-asset** task. We do this to highlight the strengths of DQN in discrete action spaces and show how different agents are suited to different problems.\n",
    "5.  **Backtesting & Comparison:** Evaluate both trained agents on out-of-sample data and compare their performance metrics and equity curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Ingestion with DuckDB\n",
    "\n",
    "First, we need data. We'll start by ingesting the `RAW_DIR` directory filled with monthly `csv.gz` files for multiple assets using our `ingest_raw_data_to_duckdb` function to build a persistent, columnar database named `market_data.duckdb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import shutil\n",
    "\n",
    "from rl_trading_project.data.duckdb_loader import ingest_raw_data_to_duckdb\n",
    "\n",
    "RAW_DIR = '../AlphaVantage Data/raw'\n",
    "DB_PATH = 'market_data.duckdb'\n",
    "\n",
    "ingest_summary = ingest_raw_data_to_duckdb(raw_dir=RAW_DIR, db_path=DB_PATH, source_timezone='UTC')\n",
    "print(\"\\n--- Ingestion Summary ---\")\n",
    "print(ingest_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load Data and Set Up Environments\n",
    "\n",
    "With our database created, we can now easily query the data we need for our training and testing periods. We'll load all the data and preprocess to make sure all assets have enough history to create a portfolio. 2/3rd is used for training and 1/3rd for backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the database and load all data into pandas\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "portfolio_df = con.execute(\"SELECT * FROM ohlcv ORDER BY timestamp, asset\").fetchdf()\n",
    "con.close()\n",
    "\n",
    "# Convert timestamp to timezone-aware and set the multi-index required by PortfolioEnv\n",
    "portfolio_df['timestamp'] = pd.to_datetime(portfolio_df['timestamp']).dt.tz_convert('UTC')\n",
    "portfolio_df = portfolio_df.set_index(['timestamp', 'asset'])\n",
    "\n",
    "print(f\"Loaded {len(portfolio_df)} total rows for assets: {portfolio_df.index.get_level_values('asset').unique().tolist()}\")\n",
    "print(\"Data Head:\")\n",
    "print(portfolio_df.head())\n",
    "\n",
    "# Visualize the data\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "for asset in portfolio_df.index.get_level_values('asset').unique():\n",
    "    asset_prices = portfolio_df.xs(asset, level='asset')['close']\n",
    "    ax.plot(asset_prices, label=asset)\n",
    "ax.set_title('Loaded Asset Price History')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Price')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Data Filtering and Splitting ---\n",
    "# In a real-world scenario, you first filter for assets with sufficient history.\n",
    "# We will demonstrate this by filtering for assets with at least 23 months of data.\n",
    "MIN_MONTHS = 23\n",
    "month_counts = portfolio_df.reset_index().groupby('asset')['timestamp'].apply(lambda x: (x.max() - x.min()).days / 30.44)\n",
    "assets_with_enough_data = month_counts[month_counts >= MIN_MONTHS].index.tolist()\n",
    "\n",
    "print(f\"Assets with >= {MIN_MONTHS} months of data: {assets_with_enough_data}\")\n",
    "\n",
    "# Filter the main DataFrame to only include these assets\n",
    "portfolio_df = portfolio_df[portfolio_df.index.get_level_values('asset').isin(assets_with_enough_data)]\n",
    "\n",
    "# CRITICAL: Check if any assets remain after filtering before proceeding.\n",
    "if portfolio_df.empty:\n",
    "    raise ValueError(\"No assets met the minimum data requirement. Cannot proceed with training.\")\n",
    "\n",
    "# Get unique timestamps for the filtered set of assets\n",
    "unique_timestamps = portfolio_df.index.get_level_values('timestamp').unique().sort_values()\n",
    "\n",
    "# Calculate the split point (2/3 for training, 1/3 for testing)\n",
    "split_index = int(len(unique_timestamps) * (2/3))\n",
    "split_date = unique_timestamps[split_index]\n",
    "\n",
    "print(f\"Total unique timestamps: {len(unique_timestamps)}\")\n",
    "print(f\"Calculated split date for 2/3 train, 1/3 test: {split_date}\")\n",
    "\n",
    "# Split the DataFrame based on the calculated timestamp\n",
    "train_df = portfolio_df[portfolio_df.index.get_level_values('timestamp') < split_date]\n",
    "test_df = portfolio_df[portfolio_df.index.get_level_values('timestamp') >= split_date]\n",
    "\n",
    "print(f\"\\nTraining data from {train_df.index.get_level_values('timestamp').min()} to {train_df.index.get_level_values('timestamp').max()}\")\n",
    "print(f\"Testing data from {test_df.index.get_level_values('timestamp').min()} to {test_df.index.get_level_values('timestamp').max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train the PPO Agent for Multi-Asset Portfolio Management\n",
    "\n",
    "PPO is perfectly suited for our `PortfolioEnv` because its action space is a continuous vector representing the target allocation for each asset. We will train it on our multi-assset portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_trading_project.agents import PPOAgent\n",
    "from rl_trading_project.envs import PortfolioEnv, GymWrapper\n",
    "import torch\n",
    "\n",
    "# --- PPO Setup ---\n",
    "SEED = 42\n",
    "WINDOW_SIZE = 30\n",
    "\n",
    "# Create the multi-asset environment with the training data\n",
    "ppo_env = PortfolioEnv(\n",
    "    df=train_df,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    initial_balance=100_000,\n",
    "    max_leverage=2.0,\n",
    "    commission=0.0005, # 5 bps\n",
    "    reward_type='risk_adjusted',\n",
    "    drawdown_penalty=100.0\n",
    ")\n",
    "ppo_wrapped_env = GymWrapper(ppo_env)\n",
    "\n",
    "# Instantiate the PPO agent\n",
    "ppo_agent = PPOAgent(\n",
    "    obs_dim=ppo_wrapped_env.observation_space.shape[0],\n",
    "    action_dim=ppo_wrapped_env.action_space.shape[0],\n",
    "    lr=3e-4, \n",
    "    epochs=10,\n",
    "    minibatch_size=128,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# --- PPO Training Loop ---\n",
    "TRAIN_STEPS = 50000 \n",
    "ROLLOUT_LEN = 512\n",
    "\n",
    "print(\"Starting PPO training...\")\n",
    "obs, _ = ppo_wrapped_env.reset(start_index=WINDOW_SIZE)\n",
    "trajectories = []\n",
    "total_steps_done = 0\n",
    "\n",
    "while total_steps_done < TRAIN_STEPS:\n",
    "    # Rollout Phase\n",
    "    for _ in range(ROLLOUT_LEN):\n",
    "        action, logp, value = ppo_agent.act(obs, deterministic=False)\n",
    "        next_obs, reward, terminated, truncated, info = ppo_wrapped_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        trajectories.append({'obs': obs, 'act': action, 'rew': reward, 'done': done, 'logp': logp, 'value': value})\n",
    "        obs = next_obs\n",
    "        total_steps_done += 1\n",
    "        if done: \n",
    "            obs, _ = ppo_wrapped_env.reset(start_index=WINDOW_SIZE)\n",
    "        if total_steps_done >= TRAIN_STEPS: break\n",
    "\n",
    "    # Update Phase\n",
    "    stats = ppo_agent.update(trajectories)\n",
    "    trajectories.clear()\n",
    "    if total_steps_done % (ROLLOUT_LEN * 10) == 0: # Print less frequently\n",
    "        print(f\"Step: {min(total_steps_done, TRAIN_STEPS)}/{TRAIN_STEPS}, Policy Loss: {stats['policy_loss']:.4f}, Value Loss: {stats['value_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nPPO Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the DQN Agent for Single-Asset Trading\n",
    "\n",
    "Our `DuelingDQNAgent` is a value-based agent designed for problems with a **discrete action space**. It outputs a Q-value for each possible action (e.g., full short, half short, hold, half long, full long). This is fundamentally different from PPO, which can output a continuous action vector.\n",
    "\n",
    "Therefore, we cannot directly apply our DQN agent to the multi-asset `PortfolioEnv`. Instead, we will train it on a simplified, **single-asset** version of the environment using only the `QQQ` data. This is a common and powerful use case for DQN-style agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_trading_project.agents import DuelingDQNAgent\n",
    "from rl_trading_project.envs import TradingEnv # Using SimpleEnv for clarity, but a single-asset PortfolioEnv also works\n",
    "\n",
    "# --- DQN Setup ---\n",
    "# Filter the training data for our single asset\n",
    "qqq_train_df = train_df.xs('QQQ', level='asset').reset_index()\n",
    "\n",
    "# Create a single-asset environment\n",
    "dqn_env = TradingEnv(\n",
    "    df=qqq_train_df,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    initial_balance=100_000,\n",
    "    max_position=100.0, # Max position in units of the asset\n",
    "    commission=0.0005\n",
    ")\n",
    "dqn_wrapped_env = GymWrapper(dqn_env)\n",
    "\n",
    "# Instantiate the DQN agent\n",
    "dqn_agent = DuelingDQNAgent(\n",
    "    obs_dim=dqn_wrapped_env.observation_space.shape[0],\n",
    "    action_bins=11, # Discretize action into 11 bins from -1 (full short) to +1 (full long)\n",
    "    lr=5e-4,\n",
    "    buffer_size=100_000,\n",
    "    batch_size=128,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# --- DQN Training Loop ---\n",
    "print(\"\\nStarting DQN training...\")\n",
    "obs, _ = dqn_wrapped_env.reset(start_index=WINDOW_SIZE)\n",
    "\n",
    "for step in range(TRAIN_STEPS):\n",
    "    action = dqn_agent.act(obs, deterministic=False)\n",
    "    next_obs, reward, terminated, truncated, info = dqn_wrapped_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    dqn_agent.add_experience(obs, action, reward, next_obs, done)\n",
    "    stats = dqn_agent.update(sync_freq=200)\n",
    "    \n",
    "    obs = next_obs\n",
    "    if done:\n",
    "        obs, _ = dqn_wrapped_env.reset(start_index=WINDOW_SIZE)\n",
    "        \n",
    "    if (step + 1) % 2000 == 0:\n",
    "         print(f\"Step {step+1}/{TRAIN_STEPS}, Loss={stats.get('loss'):.4f}, Epsilon={stats.get('eps'):.2f}\")\n",
    "\n",
    "print(\"DQN Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Backtesting and Comparison\n",
    "\n",
    "Now for the moment of truth. We will use the `Backtester` module to run both of our trained agents on the out-of-sample test data (the 3rd month). For the evaluation, we always use `deterministic=True` to make the agent exploit its learned policy without random exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_trading_project.trainers import Backtester, compare_strategies, reporting\n",
    "\n",
    "# --- 1. PPO Backtest (Multi-Asset) ---\n",
    "def ppo_policy_fn(obs, t):\n",
    "    action, _, _ = ppo_agent.act(obs, deterministic=True)\n",
    "    return action\n",
    "\n",
    "ppo_test_env_factory = lambda: GymWrapper(PortfolioEnv(df=test_df, window_size=WINDOW_SIZE))\n",
    "\n",
    "print(\"Running PPO backtest...\")\n",
    "ppo_backtester = Backtester(ppo_test_env_factory, start_index=WINDOW_SIZE)\n",
    "ppo_results = ppo_backtester.run(ppo_policy_fn, max_steps=len(test_df.index.get_level_values('timestamp').unique()) - WINDOW_SIZE - 1)\n",
    "\n",
    "# --- 2. DQN Backtest (Single-Asset on STCK) ---\n",
    "def dqn_policy_fn(obs, t):\n",
    "    action = dqn_agent.act(obs, deterministic=True)\n",
    "    return action\n",
    "\n",
    "qqq_test_df = test_df.xs('QQQ', level='asset').reset_index()\n",
    "dqn_test_env_factory = lambda: GymWrapper(SimpleEnv(df=qqq_test_df, window_size=WINDOW_SIZE))\n",
    "\n",
    "print(\"Running DQN backtest...\")\n",
    "dqn_backtester = Backtester(dqn_test_env_factory, start_index=WINDOW_SIZE)\n",
    "dqn_results = dqn_backtester.run(dqn_policy_fn, max_steps=len(qqq_test_df) - WINDOW_SIZE - 1)\n",
    "\n",
    "# --- 3. Comparison ---\n",
    "comparison = compare_strategies({\n",
    "    'PPO_MultiAsset': ppo_results,\n",
    "    'DQN_SingleAsset': dqn_results\n",
    "})\n",
    "\n",
    "print(\"\\n--- Backtest Comparison Summary ---\")\n",
    "summary_df = pd.DataFrame(comparison).T[['total_return', 'sharpe_ratio', 'max_drawdown', 'end_value']]\n",
    "summary_df['total_return'] = summary_df['total_return'].apply(lambda x: f\"{x:.2%}\")\n",
    "summary_df['max_drawdown'] = summary_df['max_drawdown'].apply(lambda x: f\"{x:.2%}\")\n",
    "print(summary_df)\n",
    "\n",
    "# --- 4. Plotting Equity Curves ---\n",
    "ppo_history_df = pd.DataFrame(ppo_results['history'])\n",
    "dqn_history_df = pd.DataFrame(dqn_results['history'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(ppo_history_df['total_value'], label='PPO (Multi-Asset Portfolio)', lw=2)\n",
    "ax.plot(dqn_history_df['total_value'], label='DQN (Single-Asset: STCK)', lw=2)\n",
    "ax.set_title('Agent Equity Curves (Out-of-Sample)')\n",
    "ax.set_xlabel('Test Steps')\n",
    "ax.set_ylabel('Portfolio Value ($)')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the full workflow: from raw data ingestion to training multiple, distinct RL agents and comparing their performance. \n",
    "\n",
    "The **PPO agent** successfully learned a policy to manage a portfolio of correlated assets, leveraging its ability to output continuous allocation vectors. \n",
    "\n",
    "The **DQN agent**, while not suitable for the multi-asset task in its current form, proved effective for a single-asset trading problem with a discretized set of actions (buy/sell/hold decisions). \n",
    "\n",
    "This highlights a key principle in applied RL: choosing the right agent architecture for the specific problem and action space is critical for success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
